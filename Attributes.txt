Q1
# ASS1.py
from pathlib import Path
import pandas as pd

# Option A - if csv is in the same project folder
DATA = Path(__file__).resolve().parent / "Titanic.csv"
# Option B - absolute path (use raw string or forward slashes)
# DATA = Path(r"C:\Users\Shravani\Desktop\DSML Practical\Titanic.csv")

# Read CSV
df = pd.read_csv(DATA)

# Reading Data head
print("Reading Data: \n\n", df.head())

# Reading Data From Index
print("From Index 2 to 5: \n\n", df.iloc[2:6])

# Selecting columns
print("Column PassengerId and Name: \n\n", df[['PassengerId', 'Name']].head())

# Sorting the Data
print("Data Sorted by Age: \n\n", df.sort_values(by='Age').head())

# Describing the Data
print("Describing the data: \n\n", df.describe(include='all'))

# Checking Dtypes
print("Datatype of Each Column: \n\n", df.dtypes)


Q2
import pandas as pd

# -----------------------------
# Load dataset from CSV
# -----------------------------
df = pd.read_csv("Telecom Churn.csv")

print("First 5 rows of dataset:\n")
print(df.head())

# -----------------------------
# Selecting only numeric columns
# -----------------------------
numeric_df = df.select_dtypes(include=["int64", "float64"])

print("\nNumeric Columns:")
print(numeric_df.columns)

# -----------------------------
# 1. Minimum value of each feature
# -----------------------------
print("\nMinimum value of each feature:")
print(numeric_df.min())

# -----------------------------
# 2. Maximum value of each feature
# -----------------------------
print("\nMaximum value of each feature:")
print(numeric_df.max())

# -----------------------------
# 3. Mean of each feature
# -----------------------------
print("\nMean of each feature:")
print(numeric_df.mean())

# -----------------------------
# 4. Range = Max - Min
# -----------------------------
print("\nRange of each feature:")
range_values = numeric_df.max() - numeric_df.min()
print(range_values)

# -----------------------------
# 5. Standard Deviation
# -----------------------------
print("\nStandard Deviation:")
print(numeric_df.std())

# -----------------------------
# 6. Variance
# -----------------------------
print("\nVariance:")
print(numeric_df.var())

# -----------------------------
# 7. Percentiles
# -----------------------------
print("\nPercentiles (25%, 50%, 75%):")
print(numeric_df.quantile([0.25, 0.50, 0.75]))


Q3
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

# -----------------------------------
# Load the real dataset from CSV
# -----------------------------------
df = pd.read_csv("House Data.csv")    # üëà use exact file name seen in VS Code

print("Dataset Loaded Successfully\n")
print(df.head())

# -----------------------------------
# Select only numeric columns
# -----------------------------------
numeric_df = df.select_dtypes(include='number')
print("\nNumeric Columns:")
print(numeric_df.columns)

# -----------------------------------
# 1. Standard Deviation
# -----------------------------------
print("\nStandard Deviation:")
std_dev = numeric_df.std()
print(std_dev)

# -----------------------------------
# 2. Variance
# -----------------------------------
print("\nVariance:")
variance = numeric_df.var()
print(variance)

# -----------------------------------
# 3. Percentiles (25%, 50%, 75%)
# -----------------------------------
print("\nPercentiles (25%, 50%, 75%):")
percentiles = numeric_df.quantile([0.25, 0.50, 0.75])
print(percentiles)

# -----------------------------------
# 4. Histograms for all numeric features
# -----------------------------------
numeric_df.hist(bins=15, figsize=(12, 8))
plt.suptitle("Histograms of House Dataset Features", fontsize=14)
plt.tight_layout()

# Save histogram image (safe for Linux exam)
plt.savefig("house_price_histograms.png")

# If GUI available, you can show the plot
# plt.show()
print("\nHistogram image saved as: house_price_histograms.png")

Q4
import pandas as pd
import math
# 1. Load dataset from CSV
df = pd.read_csv("Lipstick.csv")  # üëà use exact file name shown in VS Code

# Convert dataframe rows into list of dictionaries (same format as before)
data = df.to_dict(orient="records")

target_attr = "Buys"   # target variable

# 2. Helper: get the list of values of a column
def get_column(data, attr):
    return [row[attr] for row in data]

# 3. Compute entropy of a list of class labels
def entropy(class_values):
    total = len(class_values)
    value_counts = {}
    for v in class_values:
        value_counts[v] = value_counts.get(v, 0) + 1

    ent = 0.0
    for count in value_counts.values():
        p = count / total
        ent -= p * math.log2(p)
    return ent

# 4. Compute Information Gain of an attribute
def information_gain(data, attr, target_attr):
    total_entropy = entropy(get_column(data, target_attr))
    total_len = len(data)

    values = set(get_column(data, attr))

    weighted_entropy = 0.0
    for v in values:
        subset = [row for row in data if row[attr] == v]
        subset_labels = get_column(subset, target_attr)
        subset_entropy = entropy(subset_labels)
        weight = len(subset) / total_len
        weighted_entropy += weight * subset_entropy

    ig = total_entropy - weighted_entropy
    return ig

# 5. Find attribute with maximum information gain
def find_best_attribute(data, target_attr):
    attributes = list(data[0].keys())
    attributes.remove(target_attr)

    best_attr = None
    best_ig = -1

    print("Information Gain for each attribute:\n")
    for attr in attributes:
        ig = information_gain(data, attr, target_attr)
        print(f"IG({attr}) = {ig:.4f}")
        if ig > best_ig:
            best_ig = ig
            best_attr = attr

    return best_attr, best_ig
# 6. Main: compute root node
best_attr, best_ig = find_best_attribute(data, target_attr)

print("\n====================================")
print(" ROOT NODE of the decision tree is:")
print("  Attribute:", best_attr)
print("  Information Gain:", best_ig)
print("====================================")

Q5
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# 1. Load dataset from CSV file
df = pd.read_csv("Lipstick.csv")
print("Loaded dataset (first 5 rows):")
print(df.head())
# 2. Separate features (X) and target (y)
X = df[["Age", "Income", "Gender", "Ms"]]   # Ms = Marital Status
y = df["Buys"]
# 3. One-hot encode categorical features
X_encoded = pd.get_dummies(X)

print("\nEncoded training data:")
print(X_encoded.head())

# 4. Train Decision Tree
clf = DecisionTreeClassifier(criterion="entropy", random_state=0)
clf.fit(X_encoded, y)
# 5. Test sample given in question:
#    Age <21, Income = Low, Gender = Female, MaritalStatus = Married
test_sample = pd.DataFrame(
    [[ "<21", "Low", "Female", "Married" ]],
    columns=["Age", "Income", "Gender", "Ms"]
)
# One-hot encode and align with training columns
test_encoded = pd.get_dummies(test_sample)
test_encoded = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)

print("\nEncoded test sample:")
print(test_encoded)
# 6. Prediction
prediction = clf.predict(test_encoded)[0]

print("\nDecision for the test data:")
print("Buys =", prediction)

Q6
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
# 1. Load lipstick dataset
df = pd.read_csv("Lipstick.csv")   # your real dataset
print("Training Dataset:")
print(df.head())
# 2. Separate features and target
X = df[["Age", "Income", "Gender", "Ms"]]   # Ms is Marital Status
y = df["Buys"]
# 3. Encode categorical features
X_encoded = pd.get_dummies(X)
# 4. Train Decision Tree
model = DecisionTreeClassifier(criterion="entropy")
model.fit(X_encoded, y)

# 5. Test Data for Question 6
test_data = pd.DataFrame(
    [[">35", "Medium", "Female", "Married"]],
    columns=["Age", "Income", "Gender", "Ms"]
)
# Encode test data and align with training columns
test_encoded = pd.get_dummies(test_data)
test_encoded = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)
# 6. Prediction
prediction = model.predict(test_encoded)[0]

print("\nPrediction for Test Data [Age>35, Income=Medium, Gender=Female, Married]:")
print("Buys =", prediction)

Q7
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# ------------------------------------
# 1. Load the lipstick dataset
# ------------------------------------
df = pd.read_csv("Lipstick.csv")

print("Training Dataset (first 5 rows):")
print(df.head())
# 2. Separate features and target
X = df[["Age", "Income", "Gender", "Ms"]]   # Ms = Marital Status
y = df["Buys"]
# 3. Encode categorical variables
X_encoded = pd.get_dummies(X)

print("\nEncoded Training Features:")
print(X_encoded)
# 4. Build Decision Tree model
clf = DecisionTreeClassifier(criterion="entropy", random_state=0)
clf.fit(X_encoded, y)
# 5. Test data for Question 7:
#    Age > 35, Income = Medium, Gender = Female, Marital Status = Married
#    ‚Üí Use Age = ">35"
test_sample = pd.DataFrame(
    [[">35", "Medium", "Female", "Married"]],
    columns=["Age", "Income", "Gender", "Ms"]
)
# Encode and align test sample
test_encoded = pd.get_dummies(test_sample)
test_encoded = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)

print("\nEncoded Test Sample:")
print(test_encoded)

# 6. Prediction
prediction = clf.predict(test_encoded)[0]
print("\nDecision for test data [Age>35, Income=Medium, Gender=Female, Married]:")
print("Buys =", prediction)

Q8
iimport pandas as pd
from sklearn.tree import DecisionTreeClassifier
# 1. Load real lipstick dataset (from college)
df = pd.read_csv("Lipstick.csv")   # make sure file is in same folder
print("Training Dataset (first 5 rows):\n")
print(df.head())
# 2. Separate features and target
X = df[["Age", "Income", "Gender", "Ms"]]   # Ms = Marital Status
y = df["Buys"]
# 3. Convert categorical features ‚Üí numeric
X_encoded = pd.get_dummies(X)
print("\nEncoded Training Data:\n")
print(X_encoded)
# 4. Train Decision Tree
model = DecisionTreeClassifier(criterion="entropy", random_state=0)
model.fit(X_encoded, y)
# 5. Test data for Question 8
#    [Age = 21‚Äì35, Income = Low, Gender = Male, Marital Status = Married]
test_data = pd.DataFrame(
    [["21-35", "Low", "Male", "Married"]],
    columns=["Age", "Income", "Gender", "Ms"]
)
# One-hot encode and align with training columns
test_encoded = pd.get_dummies(test_data)
test_encoded = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)
print("\nEncoded Test Data:\n")
print(test_encoded)
# 6. Prediction
prediction = model.predict(test_encoded)[0]
print("\nDecision for test data [Age=21-35, Income=Low, Gender=Male, Married]:")
print("Buys =", prediction)

Q9
import math
# Given points
points = {
    "P1": [0.1, 0.6],
    "P2": [0.15, 0.71],
    "P3": [0.08, 0.9],
    "P4": [0.16, 0.85],
    "P5": [0.2, 0.3],
    "P6": [0.25, 0.5],
    "P7": [0.24, 0.1],
    "P8": [0.3, 0.2]
}
# Initial cluster centers
m1 = points["P1"]   # Cluster C1
m2 = points["P8"]   # Cluster C2
# Euclidean distance function
def euclidean(p, q):
    return math.sqrt((p[0] - q[0])**2 + (p[1] - q[1])**2)
# Clusters
C1 = []
C2 = []
# Step 1: Assign points to nearest cluster
for name, point in points.items():
    d1 = euclidean(point, m1)
    d2 = euclidean(point, m2)
    
    if d1 <= d2:
        C1.append(name)
    else:
        C2.append(name)

# Step 2: Print clusters
print("Cluster 1 (C1) points:", C1)
print("Cluster 2 (C2) points:", C2)
# Step 3: Check where P6 belongs
if "P6" in C1:
    print("\nP6 belongs to Cluster 1 (C1)")
else:
    print("\nP6 belongs to Cluster 2 (C2)")
# Step 4: Population of cluster around m2
print("Population of cluster around m2 (C2) =", len(C2))
# Step 5: Update centroids
def calculate_centroid(cluster_points):
    x_sum = 0
    y_sum = 0
    for name in cluster_points:
        p = points[name]
        x_sum += p[0]
        y_sum += p[1]
    return [x_sum / len(cluster_points), y_sum / len(cluster_points)]
new_m1 = calculate_centroid(C1)
new_m2 = calculate_centroid(C2)
print("\nUpdated centroid m1 =", new_m1)
print("Updated centroid m2 =", new_m2)


Q10
import math

# Given points
points = {
    "P1": [2, 10],
    "P2": [2, 5],
    "P3": [8, 4],
    "P4": [5, 8],
    "P5": [7, 5],
    "P6": [6, 4],
    "P7": [1, 2],
    "P8": [4, 9]
}

# Initial centroids
m1 = points["P1"]   # Cluster C1
m2 = points["P4"]   # Cluster C2
m3 = points["P7"]   # Cluster C3

# Euclidean distance function
def euclidean(p, q):
    return math.sqrt((p[0] - q[0])**2 + (p[1] - q[1])**2)

# Create empty clusters
C1, C2, C3 = [], [], []

# Step 1: Assign points to nearest centroid
for name, point in points.items():
    d1 = euclidean(point, m1)
    d2 = euclidean(point, m2)
    d3 = euclidean(point, m3)

    if d1 <= d2 and d1 <= d3:
        C1.append(name)
    elif d2 <= d1 and d2 <= d3:
        C2.append(name)
    else:
        C3.append(name)

# Step 2: Print clusters
print("Cluster C1:", C1)
print("Cluster C2:", C2)
print("Cluster C3:", C3)

# Step 3: Which cluster does P6 belong to?
if "P6" in C1:
    print("\nP6 belongs to Cluster C1")
elif "P6" in C2:
    print("\nP6 belongs to Cluster C2")
else:
    print("\nP6 belongs to Cluster C3")

# Step 4: Population of cluster around m3
print("Population of cluster around m3 (C3):", len(C3))

# Step 5: Function to update centroids
def update_centroid(cluster):
    x_sum, y_sum = 0, 0
    for name in cluster:
        x_sum += points[name][0]
        y_sum += points[name][1]
    return [x_sum / len(cluster), y_sum / len(cluster)]

# Calculate new centroids
new_m1 = update_centroid(C1)
new_m2 = update_centroid(C2)
new_m3 = update_centroid(C3)

print("\nUpdated centroids:")
print("New m1 =", new_m1)
print("New m2 =", new_m2)
print("New m3 =", new_m3)

Q11

import pandas as pd
import matplotlib.pyplot as plt
# 1. Load the Iris dataset from CSV
df = pd.read_csv("IRIS.csv")
print("Dataset Loaded Successfully\n")
print(df.head())
# 2. List features and their types
print("\nFeatures and their data types:\n")
print(df.dtypes)
print("\nFeature Types:")
print("sepal_length  --> Numeric (Continuous)")
print("sepal_width   --> Numeric (Continuous)")
print("petal_length  --> Numeric (Continuous)")
print("petal_width   --> Numeric (Continuous)")
print("species       --> Nominal (Categorical)")
# 3. Histograms for each numeric feature
features = ["sepal_length", "sepal_width", "petal_length", "petal_width"]
plt.figure(figsize=(10, 8))
for i, feature in enumerate(features, 1):
    plt.subplot(2, 2, i)
    plt.hist(df[feature], bins=20)
    plt.title(f"Histogram of {feature}")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig("iris_histograms.png")   # for exam PCs (in case no GUI)
# plt.show()  # only if GUI available
print("\nHistogram saved as iris_histograms.png")

Q12

import pandas as pd
import matplotlib.pyplot as plt
# 1. Load the Iris dataset from the provided CSV file
df = pd.read_csv("IRIS.csv")
print("First 5 rows of Iris dataset:")
print(df.head())
# Numeric features
features = ["sepal_length", "sepal_width", "petal_length", "petal_width"]
# 2. Create Box Plot for each numeric feature
plt.figure(figsize=(10, 8))
for i, feature in enumerate(features, 1):
    plt.subplot(2, 2, i)
    plt.boxplot(df[feature])
    plt.title(f"Box Plot of {feature}")
    plt.ylabel(feature)
plt.tight_layout()
plt.savefig("iris_boxplots.png")     # Saves image for lab PCs
# plt.show()                         # Show if GUI available
print("\nBox plots saved as iris_boxplots.png")
# 3. Box Plots by species (distribution comparison)
plt.figure(figsize=(10, 8))
species_list = df["species"].unique()
for i, feature in enumerate(features, 1):
    plt.subplot(2, 2, i)
    data = [df[df["species"] == s][feature] for s in species_list]
    plt.boxplot(data, tick_labels=species_list)
    plt.title(f"{feature} Distribution by Species")
    plt.ylabel(feature)
plt.tight_layout()
plt.savefig("iris_species_boxplots.png")
# plt.show()
print("Species-wise box plots saved as iris_species_boxplots.png")

Q13
import pandas as pd
# Load the dataset with correct filename
df = pd.read_csv("Covid Vaccine Statewise.csv")

# a. Describe the dataset
print("------ Dataset Info ------")
print(df.info())
print("\n------ First 5 Rows ------")
print(df.head())
print("\n------ Statistical Description ------")
print(df.describe())
print("\n------ Columns in Dataset ------")
print(df.columns)
# b. State-wise people vaccinated for First Dose
if "First Dose Administered" in df.columns:
    print("\n------ State-wise First Dose Vaccination ------")
    print(df[["State", "First Dose Administered"]])
    print("\nTotal First Dose Vaccinated in India:",
          df["First Dose Administered"].sum())
else:
    print("\nColumn 'First Dose Administered' not found!")
# c. State-wise people vaccinated for Second Dose
if "Second Dose Administered" in df.columns:
    print("\n------ State-wise Second Dose Vaccination ------")
    print(df[["State", "Second Dose Administered"]])
    print("\nTotal Second Dose Vaccinated in India:",
          df["Second Dose Administered"].sum())
else:
    print("\nColumn 'Second Dose Administered' not found!")


Q14

import pandas as pd
# Load the dataset (exact filename from your folder)
df = pd.read_csv("Covid Vaccine Statewise.csv")
print("------ Dataset Information ------")
print(df.info())
print("\n------ First 5 Rows ------")
print(df.head())
print("\n------ Statistical Description ------")
print(df.describe())
print("\n------ Columns Available ------")
print(df.columns)
# Find correct column name (case-sensitive)
possible_male_columns = [col for col in df.columns if "male" in col.lower()]
if possible_male_columns:
    male_col = possible_male_columns[0]
    total_males = df[male_col].sum()
    print(f"\nTotal Males Vaccinated ({male_col}): {total_males}")
else:
    print("\n‚ùå No 'Male' column found in dataset!")
possible_female_columns = [col for col in df.columns if "female" in col.lower()]

if possible_female_columns:
    female_col = possible_female_columns[0]
    total_females = df[female_col].sum()
    print(f"\nTotal Females Vaccinated ({female_col}): {total_females}")
else:
    print("\n‚ùå No 'Female' column found in dataset!")


Q15

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 0) Install note (run once if needed):
# pip install pandas seaborn matplotlib

# ---------------------------
# 1) Load local Titanic CSV
# ---------------------------
df = pd.read_csv("Titanic.csv")   # use exact filename in your folder
print("Loaded Titanic.csv ‚Äî shape:", df.shape)
print("Columns:", df.columns.tolist(), "\n")
print("First 5 rows:\n", df.head(), "\n")

# Normalize column names to easy lowercase keys (no spaces)
df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

# map common names to expected keys used below
# possible variations in provided CSVs: survived/Survived, sex/Sex, pclass/Pclass, age/Age, fare/Fare
col_map = {
    "survived": None,
    "sex": None,
    "pclass": None,
    "age": None,
    "fare": None
}
# find which normalized column matches each key
for key in list(col_map.keys()):
    for c in df.columns:
        if key == c or key in c:
            col_map[key] = c
            break

print("Column mapping (detected):", col_map)

# verify required columns exist
missing = [k for k,v in col_map.items() if v is None]
if missing:
    raise SystemExit(f"Missing required columns in Titanic.csv: {missing}. Please check file columns.")

# convenience variables
surv_col = col_map["survived"]
sex_col  = col_map["sex"]
pclass_col = col_map["pclass"]
age_col = col_map["age"]
fare_col = col_map["fare"]

# ---------------------------
# 2) Plot: Survival count
# ---------------------------
plt.figure(figsize=(6,4))
sns.countplot(x=surv_col, data=df)
plt.title("Survival Count (0 = No, 1 = Yes)")
plt.xlabel("survived")
plt.ylabel("count")
plt.tight_layout()
plt.savefig("titanic_survival_count.png")
plt.close()

# ---------------------------
# 3) Survival by gender
# ---------------------------
plt.figure(figsize=(6,4))
sns.countplot(x=sex_col, hue=surv_col, data=df)
plt.title("Survival by Gender")
plt.tight_layout()
plt.savefig("titanic_survival_by_gender.png")
plt.close()

# ---------------------------
# 4) Survival by passenger class
# ---------------------------
plt.figure(figsize=(6,4))
sns.countplot(x=pclass_col, hue=surv_col, data=df)
plt.title("Survival by Passenger Class")
plt.tight_layout()
plt.savefig("titanic_survival_by_pclass.png")
plt.close()

# ---------------------------
# 5) Age distribution (hist)
# ---------------------------
plt.figure(figsize=(7,4))
sns.histplot(df[age_col].dropna(), bins=30, kde=True)
plt.title("Age Distribution of Passengers")
plt.xlabel("Age")
plt.tight_layout()
plt.savefig("titanic_age_distribution.png")
plt.close()

# ---------------------------
# 6) Survival vs Age (boxplot)
# ---------------------------
plt.figure(figsize=(6,4))
sns.boxplot(x=surv_col, y=age_col, data=df)
plt.title("Age by Survival")
plt.tight_layout()
plt.savefig("titanic_age_by_survival.png")
plt.close()

# ---------------------------
# 7) Fare distribution
# ---------------------------
plt.figure(figsize=(7,4))
sns.histplot(df[fare_col].dropna(), bins=40, kde=True)
plt.title("Fare Distribution")
plt.xlabel("Fare")
plt.tight_layout()
plt.savefig("titanic_fare_distribution.png")
plt.close()

# ---------------------------
# 8) Correlation heatmap (numeric-only)
# ---------------------------
num = df.select_dtypes(include="number")
plt.figure(figsize=(6,5))
sns.heatmap(num.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation heatmap (numeric features)")
plt.tight_layout()
plt.savefig("titanic_correlation_heatmap.png")
plt.close()

# ---------------------------
# 9) Pairplot for selected numeric features (smaller, faster)
# ---------------------------
pair_vars = [col for col in [age_col, fare_col, pclass_col] if col in df.columns]
if pair_vars:
    sns.pairplot(df.dropna(subset=pair_vars + [surv_col]), vars=pair_vars, hue=surv_col, corner=True)
    plt.savefig("titanic_pairplot_selected.png")
    plt.close()

# ---------------------------
# 10) Summary printed to console
# ---------------------------
print("Saved plots:")
print(" - titanic_survival_count.png")
print(" - titanic_survival_by_gender.png")
print(" - titanic_survival_by_pclass.png")
print(" - titanic_age_distribution.png")
print(" - titanic_age_by_survival.png")
print(" - titanic_fare_distribution.png")
print(" - titanic_correlation_heatmap.png")
print(" - titanic_pairplot_selected.png (if pairwise variables exist)\n")

print("Numeric features summary:\n", num.describe())



Q16
import seaborn as sns
import matplotlib.pyplot as plt

# Load the built-in Titanic dataset
titanic = sns.load_dataset("titanic")

# Check number of rows
print("Total rows in dataset:", len(titanic))

# Plot histogram for 'fare'
plt.figure(figsize=(8, 6))
plt.hist(titanic["fare"], bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Ticket Fare")
plt.ylabel("Number of Passengers")
plt.title("Distribution of Ticket Fare in Titanic Dataset")
plt.show()


Q18
import pandas as pd
import numpy as np

# 1. PRINT ALL COLUMN NAMES (so you can pick groups)
df_cols = pd.read_csv("House Data.csv", nrows=0).columns.tolist()
print("--------- COLUMN NAMES IN DATASET ---------")
for c in df_cols:
    print("-", c)

# 2. SELECT CATEGORICAL + NUMERIC COLUMN (EDIT HERE)
# üëâ After running once, replace these with correct names
cat_col = "district"      # categorical column (example)
price_col = "price"       # column containing numeric values (but stored as text)
print("\nUsing Categorical Column:", cat_col)
print("Using Numeric Column:", price_col)
# 3. LOAD ONLY THE NEEDED COLUMNS
df = pd.read_csv("House Data.csv", usecols=[cat_col, price_col])
print("\n--------- FIRST 5 ROWS ---------")
print(df.head())
# 4. CLEAN PRICE COLUMN (remove commas, TL, spaces, symbols)
def clean_price(x):
    if pd.isna(x):
        return np.nan
    s = str(x)
    s = (
        s.replace(",", "")
         .replace("TL", "")
         .replace("‚Ç∫", "")
         .replace("$", "")
         .replace(" ", "")
    )
    # keep digits + decimal only
    s = ''.join(ch for ch in s if ch.isdigit() or ch == ".")
    try:
        return float(s)
    except:
        return np.nan
df["Price_Clean"] = df[price_col].apply(clean_price)
print("\n--------- CLEANED PRICE PREVIEW ---------")
print(df[[cat_col, price_col, "Price_Clean"]].head())
# ------------------------------------------------------
# 5. GROUPED SUMMARY STATISTICS
# ------------------------------------------------------
summary = df.groupby(cat_col, observed=False)["Price_Clean"].agg(
    Mean="mean",
    Median="median",
    Minimum="min",
    Maximum="max",
    Std_Dev="std",
    Count="count"
).reset_index()
print("\n--------- SUMMARY STATISTICS GROUPED BY", cat_col.upper(), "---------")
print(summary)
# ------------------------------------------------------
# 6. OPTIONAL: SAVE TO CSV
# ------------------------------------------------------
summary.to_csv("summary_house_price_grouped.csv", index=False)
print("\nSummary saved as: summary_house_price_grouped.csv")


Q19
import pandas as pd

# Load the iris dataset
df = pd.read_csv("iris.csv")

print("Full Dataset Loaded:")
print(df.head())

# Select only numeric columns
numeric_columns = ["sepal_length", "sepal_width", "petal_length", "petal_width"]

# List of required species
species_list = ["Iris-setosa", "Iris-versicolor", "Iris-virginica"]

# Loop through each species
for sp in species_list:
    print("\n==============================")
    print("Statistics for:", sp)
    print("==============================")

    # Filter dataset for the current species
    species_data = df[df["species"] == sp]

    # Select only numeric columns
    species_numeric = species_data[numeric_columns]

    # Mean
    print("\nMean Values:")
    print(species_numeric.mean())

    # Standard Deviation
    print("\nStandard Deviation:")
    print(species_numeric.std())

    # Percentiles + other statistics
    print("\nStatistical Summary (Percentile, Min, Max, etc.):")
    print(species_numeric.describe())

Q20

# ASS20.py ‚Äî K-means (K=3) on Iris (10 iterations), exam-ready
import os
import numpy as np
import pandas as pd
# ---------- PARAMETERS ----------
K = 3                # number of clusters (set to 3 for this question)
NUM_ITERS = 10       # number of iterations
SEED = 0             # reproducible initialization
CSV_NAME = "IRIS.csv"  # local CSV filename (use exact file in your folder)
# --------------------------------

# ---------- LOAD DATA ----------
if os.path.exists(CSV_NAME):
    df = pd.read_csv(CSV_NAME)
    # find numeric feature columns (common iris names)
    possible_feats = [c for c in df.columns if c.lower() in (
        "sepal_length", "sepal_width", "petal_length", "petal_width")]
    if len(possible_feats) < 4:
        # attempt to pick first 4 numeric columns
        numeric_cols = df.select_dtypes(include="number").columns.tolist()
        X = df[numeric_cols[:4]].values
    else:
        X = df[possible_feats].values
else:
    # fallback (only if IRIS.csv not present) ‚Äî still fine for local testing
    from sklearn.datasets import load_iris
    iris = load_iris()
    X = iris.data.copy()

n_samples, n_features = X.shape
print(f"Data loaded: {n_samples} samples, {n_features} features")

# ---------- INIT ----------
rng = np.random.default_rng(SEED)
initial_indices = rng.choice(n_samples, size=K, replace=False)
means = X[initial_indices].astype(float)
print("\nInitial cluster means (chosen data points):")
for i, m in enumerate(means):
    print(f"  Cluster {i} mean: {m}")

# Euclidean distance
def euclidean(a, b):
    return np.linalg.norm(a - b)

# ---------- K-MEANS ITERATIONS ----------
for it in range(1, NUM_ITERS + 1):
    # Assign step
    labels = np.empty(n_samples, dtype=int)
    for idx, p in enumerate(X):
        dists = np.linalg.norm(means - p, axis=1)  # distances to all means
        labels[idx] = np.argmin(dists)

    # Update step
    new_means = means.copy()
    for k in range(K):
        members = X[labels == k]
        if len(members) > 0:
            new_means[k] = members.mean(axis=0)
        # if a cluster has no members, keep the old mean (or reinit ‚Äî not needed here)
    means = new_means
    print(f"\nAfter iteration {it}:")
    for i, m in enumerate(means):
        print(f"  Cluster {i} mean: {np.round(m, 6)}")
    # optional: break if means do not change (convergence)
    # if it > 1 and np.allclose(prev_means, means): break
    # prev_means = means.copy()
# ---------- FINAL OUTPUT ----------
print("\nFinal cluster means after", NUM_ITERS, "iterations:")
for i, m in enumerate(means):
    print(f"  Cluster {i}: {np.round(m, 6)}")


Q21

# ASS21.py ‚Äî K-means (K=4) on Iris, 10 iterations, exam-ready
import os
import numpy as np
import pandas as pd
K = 4                # number of clusters required by question
NUM_ITERS = 10       # iterate at least 10 times
SEED = 1             # reproducible initialization
CSV_NAME = "IRIS.csv"  # local CSV filename (exact name in your folder)

# ---------- LOAD DATA ----------
if os.path.exists(CSV_NAME):
    df = pd.read_csv(CSV_NAME)
    # try to detect four numeric iris features
    possible_feats = [c for c in df.columns if c.lower() in (
        "sepal_length", "sepal_width", "petal_length", "petal_width")]
    if len(possible_feats) >= 4:
        X = df[possible_feats[:4]].values
    else:
        numeric_cols = df.select_dtypes(include="number").columns.tolist()
        if len(numeric_cols) < 4:
            raise SystemExit("IRIS.csv does not have 4 numeric columns. Check file.")
        X = df[numeric_cols[:4]].values
else:
    # fallback for local testing (won't be used in exam if IRIS.csv exists)
    from sklearn.datasets import load_iris
    iris = load_iris()
    X = iris.data.copy()

n_samples, n_features = X.shape
print(f"Data loaded: {n_samples} samples, {n_features} features")

# ---------- INITIALIZE ----------
rng = np.random.default_rng(SEED)
if K > n_samples:
    raise ValueError("K must be <= number of samples")

initial_indices = rng.choice(n_samples, size=K, replace=False)
means = X[initial_indices].astype(float)
print("\nInitial cluster means (selected data points):")
for i, m in enumerate(means):
    print(f"  Cluster {i} mean: {np.round(m,6)}")

# Euclidean distance helper
def euclidean(a, b):
    return np.linalg.norm(a - b)

# ---------- K-MEANS ITERATIONS ----------
for it in range(1, NUM_ITERS + 1):
    # ASSIGN STEP
    labels = np.empty(n_samples, dtype=int)
    for idx, p in enumerate(X):
        dists = np.linalg.norm(means - p, axis=1)  # vectorized distances
        labels[idx] = int(np.argmin(dists))

    # UPDATE STEP
    new_means = means.copy()
    for k in range(K):
        members = X[labels == k]
        if len(members) > 0:
            new_means[k] = members.mean(axis=0)
        # if a cluster has 0 members we keep the old mean (could reinit, but not required here)
    means = new_means
    print(f"\nAfter iteration {it}:")
    for i, m in enumerate(means):
        count = int((labels == i).sum())
        print(f"  Cluster {i} mean: {np.round(m,6)} (members: {count})")
# ---------- FINAL OUTPUT ----------
print("\nFinal cluster means after", NUM_ITERS, "iterations:")
for i, m in enumerate(means):
    print(f"  Cluster {i}: {np.round(m,6)}")


Q23

import pandas as pd
import numpy as np
import math
# 1. Create the dataset (from table)
data = {
    "Age": ["Young", "young", "Middle", "Old", "Old", "Old", "Middle",
            "Young", "Young", "Old", "Young", "Middle", "Middle", "Old"],
    "Income": ["High", "High", "High", "Medium", "Low", "Low", "Low",
               "Medium", "Low", "Medium", "Medium", "Medium", "High", "Medium"],
    "Married": ["No", "No", "No", "No", "Yes", "Yes", "Yes",
                "No", "Yes", "Yes", "Yes", "No", "Yes", "No"],
    "Health": ["Fair", "Good", "Fair", "Fair", "Fair", "Good", "Good",
               "Fair", "Fair", "Fair", "Good", "Good", "Fair", "Good"],
    "Class": ["No", "No", "Yes", "Yes", "Yes", "No", "Yes",
              "No", "Yes", "Yes", "Yes", "Yes", "Yes", "No"]
}
df = pd.DataFrame(data)
# Fix case so "Young" and "young" are same
df["Age"] = df["Age"].str.capitalize()
print("Full DataFrame:\n", df, "\n")
# 2. Frequency table for Age
age_freq = df["Age"].value_counts()
print("Frequency table for Age:\n", age_freq, "\n")
# 3. Entropy function
def entropy(column):
    values, counts = np.unique(column, return_counts=True)
    ent = 0.0
    for i in range(len(values)):
        p = counts[i] / counts.sum()
        ent -= p * math.log2(p)
    return ent
# 4. Total entropy of the dataset (Class)
total_entropy = entropy(df["Class"])
print("Total Entropy (before split) =", total_entropy, "\n")
# 5. Entropy for each Age group
age_values = df["Age"].unique()
age_entropy = {}
for age in age_values:
    subset = df[df["Age"] == age]
    ent = entropy(subset["Class"])
    age_entropy[age] = ent
    print(f"Entropy for Age = {age}: {ent}")
print()
# 6. Weighted entropy after splitting on Age
total_records = len(df)
weighted_entropy = 0.0
for age in age_values:
    subset = df[df["Age"] == age]
    weight = len(subset) / total_records
    weighted_entropy += weight * age_entropy[age]
    print(f"Age = {age}, weight = {weight}, "
          f"subset entropy = {age_entropy[age]}, "
          f"contribution = {weight * age_entropy[age]}")
print("\nWeighted Entropy after split on Age =", weighted_entropy, "\n")
# 7. Information Gain for Age
# -----------------------------
information_gain_age = total_entropy - weighted_entropy
print("Information Gain when splitting on Age =", information_gain_age)

Q24
import pandas as pd

# 1. Load dataset
df = pd.read_csv("Lipstick.csv")
print("Original Data:\n", df.head(), "\n")

# 2. Count unique values
print("Unique values per column:\n", df.nunique(), "\n")

# 3. Column formats (data types)
print("Data types before conversion:\n", df.dtypes, "\n")

# Convert string columns ‚Üí category (categorical datatype)
for col in df.select_dtypes(include="object"):
    df[col] = df[col].astype("category")

print("Data types after conversion:\n", df.dtypes, "\n")

# 4. Identify missing values
print("Missing values per column:\n", df.isnull().sum(), "\n")

# 5. Fill missing values
#   ‚Üí numeric: fill with mean  
#   ‚Üí categorical: fill with mode
for col in df.columns:
    if df[col].dtype == "category":
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

print("Missing values after filling:\n", df.isnull().sum(), "\n")

# Final cleaned data
print("Cleaned Data Preview:\n", df.head())


Q25
import pandas as pd
# Load dataset
df = pd.read_csv("Lipstick.csv")
print("Original Data:\n", df.head())

# 1. DATA CLEANING
# Remove duplicates
df = df.drop_duplicates()
# Strip extra spaces in text columns
for col in df.select_dtypes(include="object").columns:
    df[col] = df[col].str.strip()
# Convert category text into proper format
for col in ["Age", "Income", "Gender", "Ms", "Buys"]:
    if col in df.columns:
        df[col] = df[col].str.title()
# Fill missing values
df = df.fillna({
    "Age": df["Age"].mode()[0],
    "Income": df["Income"].mode()[0],
    "Gender": df["Gender"].mode()[0],
    "Ms": df["Ms"].mode()[0],
    "Buys": df["Buys"].mode()[0],
})
# 2. DATA TRANSFORMATION
# Convert categorical to numeric (Label Encoding)
for col in ["Age", "Income", "Gender", "Ms", "Buys"]:
    if col in df.columns:
        df[col + "_code"] = df[col].astype("category").cat.codes

print("\nCleaned & Transformed Data:\n", df.head())
